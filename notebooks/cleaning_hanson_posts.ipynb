{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d6aaa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afe17c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"C:\\Users\\yaobv\\chatgpt\\hanson_again_04_18.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1f1f329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the file\n",
    "text_file = 'hanson_again_04_18.txt'\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as hanson:\n",
    "    corpus = hanson.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d255672a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting on text i inserted to standardize the \n",
    "# the splits\n",
    "split_corpus = corpus.split('XXXTHIS IS A NEW ARTICLEXXX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "440af279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# okay, most urls returned this time i scraped the page\n",
    "# fortunately, all of the posts in no_subscribe are \n",
    "# eliezer yudkowsky posts - will clean separately\n",
    "\n",
    "no_subscribe = [x for x in split_corpus if 'Subscribe' not in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bd3cb30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting on subscribe to make cleaning the data tractable\n",
    "split_subscribe = [x.split('Subscribe') for x in split_corpus if 'Subscribe' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eb1482e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting titles and authors from the 'split on subscribe' list\n",
    "all_titles = [s[0].replace('\\n','') for s in split_subscribe]\n",
    "cleaned_titles = [x.replace('Overcoming Bias', '').replace('- ', '').strip() for x in all_titles]\n",
    "cleaned_titles = [x for x in cleaned_titles if 'Epilogue:' not in x]\n",
    "\n",
    "titles = []\n",
    "authors = []\n",
    "\n",
    "for title in cleaned_titles:\n",
    "    if 'by ' in title:\n",
    "        split_title = title.split('by ')\n",
    "        title, author = split_title[0].strip(), split_title[1].strip()\n",
    "        titles.append(title)\n",
    "        authors.append(author)\n",
    "    else:\n",
    "        titles.append(title)\n",
    "        authors.append('N/A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f73aba36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the initial dataframe with titles and authors\n",
    "ta_df = pd.DataFrame({'title' : titles, 'author' : authors})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1c0a44e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lower_case(url):\n",
    "    \"\"\"\n",
    "    a function to find all strings where a lower case char is followed\n",
    "    by an upper case char without a space in between\n",
    "    \"\"\"\n",
    "    pattern = r'[a-z][A-Z]'\n",
    "    \n",
    "    # search for pattern in the text\n",
    "    match = re.search(pattern, url)\n",
    "    \n",
    "    if match:\n",
    "        return match.start()\n",
    "    else:\n",
    "        print('error')\n",
    "        return 'error'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "26c6eeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(url):\n",
    "    \"\"\"\n",
    "    a function to find the idx of where a 'html' string begins or, if there isnt one,\n",
    "    to find where a lower case char is followed by an upper case char\n",
    "    \"\"\"\n",
    "    \n",
    "    if 'html' in url:\n",
    "        idx = url.rfind('html')\n",
    "        return url[:idx+4]\n",
    "    else:\n",
    "        x = find_lower_case(url)\n",
    "        \n",
    "        if x == 'error':\n",
    "            return 'error'\n",
    "        else:\n",
    "            return url[:x+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2060f9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title(url):\n",
    "    \n",
    "    if 'html' in url:\n",
    "        idx = url.rfind('html')    \n",
    "        return url[idx+4:]\n",
    "    else:\n",
    "        x = find_lower_case(url)\n",
    "        \n",
    "        if x == 'error':\n",
    "            return 'error'\n",
    "        else:\n",
    "            return url[x+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d0bbe3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n",
      "error\n"
     ]
    }
   ],
   "source": [
    "# extracting links and titles \n",
    "urls = ta_df['title'].apply(get_url)\n",
    "titles = ta_df['title'].apply(get_title)\n",
    "\n",
    "ta_df['url'] = urls\n",
    "ta_df['title'] = titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c6834566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR 1880\n"
     ]
    }
   ],
   "source": [
    "# getting the body from each post using a string indicator of which\n",
    "# element in the split-on-subscribe has the main content\n",
    "# there is one error.\n",
    "\n",
    "bodies = []\n",
    "\n",
    "for i, s in enumerate(split_subscribe):\n",
    "    \n",
    "    try:\n",
    "        if 'readingSign in' in s[2]:\n",
    "            bodies.append(s[2])\n",
    "    \n",
    "        elif 'to Overcoming BiasBy' in s[2]:\n",
    "            bodies.append(s[1])\n",
    "        else:\n",
    "            bodies.append(s[1])\n",
    "    \n",
    "    except:\n",
    "        print(\"ERROR\", i)\n",
    "        bodies.append('ERROR')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1cde04bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ta_df['body'] = bodies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9c8d0d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_body(text):\n",
    "    \"various string funcs and reg ex operations to replace strings\"\n",
    "    \n",
    "    fixed = text.replace('ShareShare this postStatus Honestywww.overcomingbias.comCopy linkTwitterFacebookEmail', '')\n",
    "    fixed = fixed.replace('ShareShare this post', ' ')\n",
    "    fixed = fixed.replace('Continue readingSign in', '')\n",
    "    \n",
    "    # using regex to replace url\n",
    "    pattern = r\"www\\.overcomingbias\\.com\"\n",
    "    fixed = re.sub(pattern, \" \", fixed)\n",
    "    \n",
    "    fixed = fixed.replace('\\xa0', ' ')\n",
    "    fixed = fixed.replace('Copy linkTwitterFacebookEmail', ' ')\n",
    "    fixed = fixed.replace('Sign inShare this post', ' ')\n",
    "    \n",
    "    return fixed.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "96c170e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ta_df['body'] = ta_df['body'].apply(fix_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "965346e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date(text):\n",
    "    \"a function to get the date from the body\"\n",
    "    date = re.findall(r'[A-Za-z]{3} \\d{1,2}, \\d{4}', text)\n",
    "    \n",
    "    if date:\n",
    "        return date[0]\n",
    "    else:\n",
    "        return 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9db2cf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting the date from the body\n",
    "ta_df['date_string'] = ta_df['body'].apply(get_date)\n",
    "\n",
    "# dropping rows without a date\n",
    "ta_df = ta_df[ta_df['date_string'] != 'none'].copy()\n",
    "\n",
    "# converting the weirdly formatted date to pandas datetime\n",
    "ta_df['date'] = ta_df['date_string'].apply(lambda x: pd.to_datetime(x, format='%b %d, %Y'))\n",
    "\n",
    "# sorting the new dataframe and resetting the index\n",
    "ta_df.sort_values(by='date', inplace=True)\n",
    "ta_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "05afce6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting on the date to get rid of it and clean up the body further\n",
    "new_bodies_date = []\n",
    "\n",
    "for date, body in zip(ta_df['date_string'], ta_df['body']):\n",
    "    \n",
    "    body_split = body.split(date)\n",
    "    new_bodies_date.append(body_split[1])\n",
    "\n",
    "ta_df['body'] = new_bodies_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "35d6a31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reducing to the wanted cols and ordering them\n",
    "ta_df = ta_df[['date', 'author', 'title', 'body', 'url']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ebac7b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the dataframe to csv. come back to this to\n",
    "# get those entries w/o 'Subscribe'\n",
    "ta_df.to_csv('hanson_df.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
